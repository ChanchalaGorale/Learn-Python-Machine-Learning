{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c8bcfbb",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95258ac8",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff?\n",
    "\n",
    "It describes how the **error of a model** can be decomposed into three parts:\n",
    "\n",
    "$$\n",
    "\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}\n",
    "$$\n",
    "\n",
    "* **Bias**: Error due to overly simplistic assumptions in the learning algorithm. High bias can cause the model to miss relevant relations â€” **underfitting**.\n",
    "* **Variance**: Error due to the model's sensitivity to small fluctuations in the training data. High variance means the model learns noise â€” **overfitting**.\n",
    "* **Irreducible Error**: Noise or randomness in the data that can't be removed even with a perfect model.\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Understanding\n",
    "\n",
    "| Model Complexity | Bias     | Variance | Error Type   |\n",
    "| ---------------- | -------- | -------- | ------------ |\n",
    "| Low              | High     | Low      | Underfitting |\n",
    "| Medium           | Moderate | Moderate | Optimal      |\n",
    "| High             | Low      | High     | Overfitting  |\n",
    "\n",
    "---\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "The goal in machine learning is to find a model that generalizes well to **unseen data**. Too simple, and it wonâ€™t learn enough (high bias). Too complex, and it memorizes noise (high variance).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ How to Manage the Tradeoff\n",
    "\n",
    "* **Use cross-validation** to detect overfitting/underfitting.\n",
    "* **Regularization** (like L1, L2) to reduce variance.\n",
    "* **Ensemble methods** (e.g., Random Forest) to balance both.\n",
    "* **Get more data** to reduce variance.\n",
    "* **Simplify the model** to reduce variance if overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose you're fitting a model to predict housing prices:\n",
    "\n",
    "* **Linear regression** might have high bias (can't capture non-linear trends).\n",
    "* **Polynomial regression (degree 20)** might have low bias but high variance (overfits).\n",
    "* **A well-tuned Random Forest** might balance bias and variance effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab1812",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d05befd",
   "metadata": {},
   "source": [
    "Cross-validation helps detect **overfitting** and **underfitting** by evaluating a modelâ€™s performance on **unseen (held-out) data**, not just on the training set.\n",
    "\n",
    "---\n",
    "\n",
    "### Cross-Validation?\n",
    "\n",
    "Cross-validation (CV) is a **model validation technique** that splits the dataset into several parts (called \"folds\"), trains the model on a subset, and tests it on the remaining data. The most common form is **k-fold cross-validation**.\n",
    "\n",
    "---\n",
    "\n",
    "### How It Detects Overfitting and Underfitting\n",
    "\n",
    "| Scenario         | Training Error | Validation Error | Diagnosis                            |\n",
    "| ---------------- | -------------- | ---------------- | ------------------------------------ |\n",
    "| **Underfitting** | High           | High             | Model is too simple (high bias)      |\n",
    "| **Overfitting**  | Low            | High             | Model is too complex (high variance) |\n",
    "| **Good Fit**     | Lowâ€“Moderate   | Lowâ€“Moderate     | Balanced model                       |\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition\n",
    "\n",
    "1. **Underfitting (High Bias)**:\n",
    "\n",
    "   * Model performs poorly on both training and validation folds.\n",
    "   * Cannot capture the underlying patterns.\n",
    "   * CV shows **consistently high error** across all folds.\n",
    "\n",
    "2. **Overfitting (High Variance)**:\n",
    "\n",
    "   * Model performs very well on training folds but poorly on validation folds.\n",
    "   * Learns noise and specific details of training data.\n",
    "   * CV shows **large gap between training and validation scores**.\n",
    "\n",
    "3. **Just Right**:\n",
    "\n",
    "   * Training and validation scores are both good and close to each other.\n",
    "   * CV shows **low and stable error** across folds.\n",
    "\n",
    "---\n",
    "\n",
    "### Tips:\n",
    "\n",
    "* Always compare **training vs. cross-validation performance**.\n",
    "* Use **learning curves** to visualize overfitting/underfitting.\n",
    "* Adjust **model complexity** based on CV results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d31635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut, StratifiedKFold, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=200, n_features=10, noise=10)\n",
    "\n",
    "# Low bias, high variance model\n",
    "tree = DecisionTreeRegressor()\n",
    "\n",
    "# General-purpose\n",
    "# Use with: \n",
    "# - moderate to large datasets, \n",
    "# - don't need fine control over the splits\n",
    "# - shorthand for KFold(n_splits=5) if regression or StratifiedKFold(n_splits=5) if classification\n",
    "tree_scores = cross_val_score(tree, X, y, cv=5)\n",
    "\n",
    "# with Kfold:  \n",
    "# Use with: \n",
    "# - Regression tasks, \n",
    "# - class balance is not a concern, \n",
    "# - want manual control over shuffling, random state, or fold size\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "tree_scores_kf = cross_val_score(tree, X, y, cv=kf)\n",
    "\n",
    "# with LeaveOneOut\n",
    "# Use with: \n",
    "# - dataset is very small (typically <100 samples)\n",
    "# - When you want unbiased and nearly exhaustive evaluation\n",
    "# - High-stakes scenarios (e.g., biomedical applications) with few data points\n",
    "loo = LeaveOneOut()\n",
    "tree_scores_loo = cross_val_score(tree, X, y, cv=loo)\n",
    "\n",
    "# with StratifiedKFold\n",
    "# Use with: \n",
    "# - Classification tasks, especially with class imbalance\n",
    "# - maintaining the class distribution across folds is important\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "tree_scores_skf = cross_val_score(tree, X, y, cv=skf)\n",
    "\n",
    "\n",
    "# Tree model has high training score but low CV score â†’ **Overfitting**\n",
    "# Linear model has low training and CV score â†’ **Underfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da4b5f",
   "metadata": {},
   "source": [
    "### Ensemble Techniques\n",
    "\n",
    "| Technique    | Main Goal       | Bias               | Variance                 | Ideal For                                                |\n",
    "| ------------ | --------------- | ------------------ | ------------------------ | -------------------------------------------------------- |\n",
    "| **Bagging**  | Reduce variance | Lowers variance | Keeps bias similar     | High-variance, low-bias models (e.g., decision trees)    |\n",
    "| **Boosting** | Reduce bias     | Lowers bias     | Can increase variance | High-bias, low-variance models or underfitting scenarios |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a9b0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
